---
title: "STAR aligner and counts using Nextflow"
author: "Jenny L Smith"
date: "`r Sys.Date()`"
always_allow_html: true
output:
  html_document:
    theme: yeti
    highlight: breezedark
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    fig_caption: true
    df_print: paged
---

# Set-up 

```{r set-up, eval=TRUE, echo=TRUE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

```{r eval=TRUE, message=FALSE}
knitr::opts_chunk$set(
  eval = FALSE,
  tidy.opts = list(width.cutoff = 50),
  tidy = TRUE,
  fig.align = "center",
  fig.width = 10, fig.height = 10
)

options(stringsAsFactors = FALSE, max.print = 100)
table <- function(..., useNA = "ifany") base::table(..., useNA = useNA)

library(stringr)
library(magrittr)
library(dplyr)
library(tidyr)
library(tibble)
```

# Define Functions

```{r}
create_example <- function(outfile, filepath, definitions, pattern="*", single_end=FALSE){
      
  files <- tibble(files = dir(filepath, 
                                full.names = T, recursive = T, 
                              pattern = pattern)) %>%
      mutate(single_end = tolower(single_end)) %>% 
      mutate(Read = case_when(
        single_end == "true" ~ "r1",
        grepl("[_\\.]?[Rr]1.(fastq|fq).gz", files) ~  "r1",
        TRUE ~ "r2" )) %>%
      pivot_wider(names_from = Read, values_from = files) %>% 
      mutate(id = case_when(
          single_end == "true" ~ gsub("^(.+)\\.(fastq|fq).gz", "\\1", basename(r1)),
          TRUE ~ gsub("^(.+)[_\\.][Rr][12]\\.(fastq|fq).gz", "\\1", basename(r1))))
  
  definitions <- list(
    r1 = paste(": the absolute or relative filepath for the read 1 fastq in paired-end RNA-seq, or the single-end fastq file."),
    r2 = paste(": the absolute or relative filepath for the read 2 fastq in paired-end RNA-seq."),
    id = paste(": unique sample ID, no duplicates allowed in the sample sheet."),
    single_end = paste(": boolean [true/false] if the data is single-end or paired-end."))

  if (single_end){
    definitions[["r2"]] <- paste(": Not Included for single-end RNAseq")
  }

  con <- file(outfile, open="wt")
  lapply(names(definitions), function(comment)   writeLines( paste("# column",comment, definitions[[comment]]), con))
  write.csv(files, con, row.names = FALSE, quote=FALSE)
  close(con)
}
```

```{r}
create_sra_example <- function(outfile, sra_run_table, definitions, pattern="*"){
  
  sra_sample_sheet <- read.csv(sra_run_table) %>% 
    mutate(single_end=case_when(
      LibraryLayout== "SINGLE" ~ "true",
      TRUE ~ "false")) %>%
    select(id=Run,
           single_end)
  
  definitions <- list(
    id = paste(": unique sample ID, no duplicates allowed in the sample sheet."),
    single_end = (": boolean [true/false] if the data is single-end or paired-end."))
  
  con <- file(outfile, open="wt")
  lapply(names(definitions), function(comment)   writeLines( paste("# column",comment, definitions[[comment]]), con))
  write.csv(sra_sample_sheet, con, row.names = FALSE, quote=FALSE)
  close(con)
}
```


# Nextflow Overview 

```{r eval=TRUE}
knitr::include_graphics(here::here("images/nextflow_dsl2.png"))
```

# Set-up Environment 

```{bash}
conda env create -f env/nextflow_env.yaml
conda activate nextflow_env
```

If testing interactively on cybertron, you will need to request the following resources to run STAR-aligner. 

```{bash}
qsub -I -q <queue_name> -P $(project code <project_name>) -l select=1:mem=32gb:ncpus=4
```


# Find the appropriate Modules 

The information on modules can be found on github [here](https://github.com/nf-core/modules). Once you have the module installed, nf-core will also create a 'module.json' file this is required. 

```{bash}
mkdir modules
```

```{bash}
nf-core modules list
```

## FASTQC 

```{bash}
nf-core modules install fastqc
nf-core modules update fastqc
cat modules.json
```

The module file (`modules.json`) looks like this:

> {
    "name": "",
    "homePage": "",
    "repos": {
        "nf-core/modules": {
            "fastqc": {
                "git_sha": "49b18b1639f4f7104187058866a8fab33332bdfe"
            }
        }
    }
}

It's recommended to check the linting of the module as well. 

```{bash}
nf-core modules lint fastqc
```

Then get the usage information to define the input channels to run the module.

```{bash}
nf-core modules info fastqc
```

> ðŸ“¥ Inputs  
  meta  (map) : Groovy Map containing sample information e.g. [ id:'test', single_end:false ]                     
  reads (file): List of input FastQ files of size 1 and 2 for single-end and paired-end data, respectively.     

The nextflow language has a `map` type and `map` operator (built-in function). The `meta (map)` input is an associative array, described in the [documentation](https://www.nextflow.io/docs/latest/script.html) on scripting under the section "Maps". 

A copy of the nextflow documentation shows a named dictionary with a key and value. 
> Maps are used to store associative arrays or dictionaries. They are unordered collections of heterogeneous, named data:
> scores = [ "Brett":100, "Pete":"Did not finish", "Andrew":86.87934 ]
Note that each of the values stored in the map can be of a different type. Brett is an integer, Pete is a string, and Andrew is a floating-point number.
> We can access the values in a map in two main ways:
println scores["Pete"]
println scores.Pete

Notably, this out put of `nf-core modules info fastqc` is not quite informative in regards to how to format the input channel and will need to be updated. However, an [example](https://github.com/nf-core/modules/blob/master/tests/modules/fastqc/main.nf) of the input channels for official modules can be found on github. 

## STAR-Aligner

The reference manual for extra arguments to be passed to STAR aligner can be found [here](https://physiology.med.cornell.edu/faculty/skrabanek/lab/angsd/lecture_notes/STARmanual.pdf).

```{bash}
nf-core modules install star/align
nf-core modules info star/align
```

>  ðŸ“¥ Inputs        â”‚Description 
  meta  (map)       â”‚Groovy Map containing sample information e.g. [ id:'test', single_end:false ]                 
  reads  (file)     â”‚List of input FastQ files of size 1 and 2 for single-end and paired-end data, respectively.   
  index  (directory)â”‚STAR genome index
  
Each installed module will update the `modules.json` file to include new module name and its git SHA hash for the commit that corresponds to it. This allows for greatest reproducibility when using external modules from the nf-core github repo. 

Again, this does not adequetly describe the actual inputs to the `STAR_ALIGNER()` module. It also requires the `gtf`, `star_ignore_sjdbgtf`, `seq_platform`, and `seq_center`. This will need to be updated for better documentation later. 

STAR-aligner will need to be fairly customizable - so using the `ext.args` parameter will be essential. 
The final desired command line parameters will be: 

`--twopassMode Basic --twopass1readsN -1 --readFilesCommand "gunzip -c" --outSAMattributes NH HI NM MD AS nM jM jI XS --quantMode GeneCounts`

**The quant mode description:**

> Counting number of reads per gene. With --quantMode GeneCounts option STAR will count number reads per gene while mapping. A read is counted if *it overlaps (1nt or more) one and only one gene*. Both ends of the pairedend read are checked for overlaps.

https://github.com/alexdobin/STAR/blob/master/doc/STARmanual.pdf

> N_multimapping is the total number of multimapping reads, which are not counted at all, so the number is the same for each column.
N_noFeature: if a read overlaps a gene on the opposite strand, it will be counted as noFeature for one of the stranded columns, but will be counted towards the gene (and not as noFeature) in the unstranded column. Hence N_noFeature(unstranded) can be < N_noFeature(+) + N_noFeature(-).

https://github.com/alexdobin/STAR/issues/703

To trick STAR into counting how many read map to entire genes (including both introns and exons), you can use --sjdbGTFfile gencode.vM8.annotation.gtf --sjdbGTFfeatureExon gene --sjdbGTFtagExonParentTranscript gene_id --sjdbGTFtagExonParentGene gene_name .
This resulted in only 100k "no feature" reads (<10%).
http://seqanswers.com/forums/showthread.php?t=65066


## STAR Index 

```{bash}
nf-core modules install star/genomegenerate
nf-core modules info star/genomegenerate
```

Again this follows the same process where we can take a look at the inputs listed from `nf-core modules info` command, and then structure our nextflow channels in the `main.nf` script (below) to match it. This process continues for all the 

## Custom Modules 

Creating a custom module is described for the [`split_bam.py`](http://rseqc.sourceforge.net/#spilt-bam-py) command line tool, which will help with quantifying the rRNA reads (contamination) in  RNAseq samples. 

```{bash}
conda activate nextflow 
nf-core modules create 
```

Follow the prompts provided on the command line. The first one will be for the name of the `tool/subtool`, so for this pipeline I entered `rseqc/splitbam` and noted that it need the standard `meta` input shown above where the channel includes the sample ID (`id`) and (`single_end`) information. 

This created a template module directory called `splitbam.nf` (rather than main.nf above), and it lacks a `meta.yml` file, but this can be created later [**TO DO**]. Open the `splitbam.nf` and modify the shell script content under `script:` and add the appropriate commands for the `split_bam.py`, and add any additional inputs or outputs.  


# Create the nexflow script

Initialize a nextflow workflow file called `main.nf` that uses DSL2 syntax and the fastqc module that was installed. Then edit the nexflow workflow `main.nf` in the text editor to include the workflow components.

```{r}
nf_set_up <- "nextflow.enable.dsl = 2\n\ninclude { FASTQC } from './modules/nf-core/modules/fastqc/main'"
cat(nf_set_up, file = "main.nf")
```

## Processes

Next include `STAR_ALIGN`  in the same format as `FASTQC` to the nextflow `main.nf` file, such that the `main.nf` will contains the following lines:

```
include { FASTQC } from './modules/nf-core/modules/fastqc/main.nf'
include { MULTIQC } from './modules/nf-core/modules/multiqc/main.nf'
include { STAR_ALIGN } from './modules/nf-core/modules/star/align/main.nf'
include { STAR_GENOMEGENERATE } from './modules/nf-core/modules/star/genomegenerate/main.nf'
```


## Input Channels

The `main.nf` script is also the location of the [channels](https://www.nextflow.io/docs/latest/channel.html#channel-factory) that need to be defined as inputs to the processes from the module files. 

Create the Sample sheet for the input fastq files, and a metadate file with single-end or paired-end information using R, and this file will be read into the `main.nf` script as a parameter. 

The `map` operator is a function that transforms a channel, or in other words, it applies a function onto each item in the channel. Its found in detail [here](https://www.nextflow.io/docs/latest/operator.html#map). 

In this case, we will use `map` operator on a channel to create our desired `meta` input to run the fastqc module using the "paired_sample_sheet.txt" created below. 

```
meta_ch = Channel.fromPath(file(params.sample_sheet))
    .splitCsv(header: true, sep: '\t')
    .map { meta -> [ [ "id":meta["id"], "single_end":meta["single_end" ]], //meta
                     [ file(meta["r1"], checkIfExists: true), file(meta["r2"], checkIfExists: true) ] //reads
                   ]}
```

```{r}
create_sra_example(outfile= here::here("test_data/sra_sample_sheet.csv"),
                    sra_run_table=here::here("test_data/sra_run_table.txt"))

create_example(outfile = here::here("test_data/paired_end_sample_sheet.csv"),
               filepath = here::here("test_data"),
               pattern = "Sample.+.gz")

create_example(outfile = here::here("test_data/single_end_sample_sheet.csv"), 
               filepath = here::here("test_data"),
               pattern = "SRR.+", 
               single_end = TRUE)
```


## Workflow 

And we will finally define the basic workflow. And example in psuedo-code is below. Each module is a nextflow process, and stored in the `./modules` directory in the project. An example of the DSL2 concepts are described [here](https://www.nextflow.io/docs/latest/dsl2.html#process), *except* that the processes `foo` and `bar` would be saved in indivdual files and saved in the `./modules` directory in your project. Templates are available [here](https://www.nextflow.io/docs/latest/dsl2.html#module-templates). 

```
workflow {

  FASTQC(input_channel)
  STAR_ALIGN(input_channel, params.one, parms.two, param.three)
  
}
```

# Nextflow Run 

The nextflow workflow contained in the `main.nf` file is executed on the command line. To make this as reproducible as possible, as bash script can created and will need some minimal changes to run the workflow each time (such as using different nextflow profiles, described in detail below).

Here is an example of basic run script, with a profile and configuration file defined at the top. The configuration file in `nextflow.config` is the key to customizing each workflow execution. For example, processing PI_lastnameA one day and then the next processing PI_lastnameB's data, will be reflected in the config file. 

```
#!/bin/bash

set -e
DATE=$(date +%F)
NFX_CONFIG=./nextflow.config
NFX_PROFILE="cybertron_singularity"


# Nextflow run to execute the workflow 
nextflow -c ${NFX_CONFIG} run main.nf \
    -profile ${NFX_PROFILE} \
    -with-report testing_${DATE}.html \
    -with-dag testing_${DATE}.png \
    -cache TRUE \
    -resume
```

## Workflow DAG

A DAG image can be generated for each workflow by using the flag `with-dag` in the nextflow run script. More information on output file types are found [here](https://www.nextflow.io/docs/latest/tracing.html#dag-visualisation).


# Define the Nextflow Configurations

Nextflow is platform agnostic and can run locally, on an HPC, or on the cloud platform. It also supports both conda and docker/singularity containers for extra reproducibility. 

Here we'll create a `nextflow.config` file for the FASTQC module to run using the biocontainer, and using docker or singularity as the engine. 

There is a built-in `workflow` object, that contains revelant information about the workflow configurations, usch as the `projectDir`, `workDir`,`publishDir`, and the `containerEngine` used for the pipeline. This is described in the [workflow introspection section](https://www.nextflow.io/docs/latest/metadata.html).

## User Parameters

The user parameters, such as the file paths to the appropiate genomes, should be included in the `nextflow.config` file as well. This would allow the user to simply fill in the necessary parameters in the config file and then use the same run script (`main_run.sh`) and the same `main.nf` workflow each time, without having the modify them. 

The Cybertron HPC requires both a queue and a project code in order to use the PBS scheduler. These must be defined in the user params and input into the configuration file. 

```{bash}
#on cybertron to identify the project codes
project
project info
project code RSC_adhoc
```

**To Do**: Create a function that could create the config file given the appropriate filepaths.
Maybe make into a interactive shiny-app to have user input and save the file. 

```
//global parameters
params {
    // general options
    sample_sheet                = "test_data/sample_sheet.txt"
    queue                       = 'sceaq'
    project                     = '207f23bf-acb6-4835-8bfe-142436acb58c'
    outdir                      = "./test_output/"
    publish_dir_mode            = 'copy'

    //star specific params
    // Must be full filepaths for files outside the projectDir
    index                       = '/gpfs/shared_data/STAR/human_v38/STAR_2.7'
    build_index                 = false
    gtf                         = '/gpfs/shared_data/STAR/human_v38/STAR_2.7/genes.gtf'
    fasta                       = ''
    star_ignore_sjdbgtf         = false
    seq_platform                = ''
    seq_center                  = ''
}
```

## Process Execution and Compute Resources

Its also important to set the resources that a process will use with `directives`, such as `cpus`, and `errorStrategy`. The directives are described in detail [here]. The directives can be specified in the [configuration file](https://www.nextflow.io/docs/latest/config.html#scope-process) under Scope *process*, which allows for the config file to be easily edited, while the processes in the `./modules` directory can stay unmodified. 

[Implicit variables](https://www.nextflow.io/docs/latest/script.html#implicit-variables) can be set for the important aspects, like the where the `workDir` is located, or  other runtime information on the workflow being executed, as mentioned above under `workflow introspection`. 

In the nextflow.config file you will need to include a specific variable which will contain the user specific command line parameters for the module being executed, called `ext.args` (extra arguments). This is described in the [documention](https://nf-co.re/developers/modules#writing-a-new-module-reference) on how to build a new module based on nf-core standards. There is also a brief example in the Nextflow documention on [processes](https://www.nextflow.io/docs/latest/process.html#ext).


The config file will look similar to this example:
```
// Define output directory and computation resources for the FASTQC command line program 
process {
    
    publishDir = [
        path: { "${params.outdir}/${task.process.tokenize(':')[-1].tokenize('_')[0].toLowerCase()}" },
        mode: "${params.publish_dir_mode}", //not working currently -- 
        saveAs: { filename -> filename.equals('versions.yml') ? null : filename }
    ]
    workDir = "/Users/jsmi26/Downloads/temp"
    errorStrategy = "retry"
    maxRetries = 1
    
    //process specific parameters
    withName: FASTQC {
        cpus = 1
        memory = 4.GB
        ext.args = '--quiet' //specific to nf-core modules
    }
}
```

## Docker and Singularity Configs

[Docker](https://www.nextflow.io/docs/latest/config.html#scope-docker) and [singularty](https://www.nextflow.io/docs/latest/config.html#scope-singularity) configuration options are found on the nextflow docs. 

The settings for Docker (local execution only, since it won't be on the HPC) or singularity can be defined in the configuration file as well. Docker run options, like memory, CPUs, etc are desribed [here](https://docs.docker.com/engine/reference/run/).

```
//Docker container engine configs 
docker {
    temp = 'auto'
    runOptions = "--platform linux/amd64"
}

//Configs for singularity containers on cybertron
singularity {
    autoMounts = true
    cacheDir = '/home/jsmi26/singularity'
    runOptions = '--containall --no-home'
}
```

Also, singularity containers on cybertron can be used interactively. Load the singularity module first.  Then, singularity image will need to be pulled. This can be helpful for troubleshooting a error that is arising from the software in your pipeline. 

```{bash}
#Example run a container interactively on cybertron
module load singularity/3.9.9
singularity pull library://sylabsed/examples/lolcow
singularity shell --bind test_data/:/mnt_data lolcow_latest.sif
```

Or run docker container locally
```{bash}
docker pull quay.io/biocontainers/rseqc:4.0.0--py38h4a8c8d9_1
docker run -it --rm -v $PWD/test_data:/test_data quay.io/biocontainers/rseqc:4.0.0--py38h4a8c8d9_1
```

# Config Profiles 

Nextflow allows for [profiles](https://www.nextflow.io/docs/latest/config.html#config-profiles) to be created in the configuration file. This can provide an effecient mechanism for switching between the different processes executors, such as running locally with docker for testing vs running localling on cybertron with singularity for testing.

The final profile, `PBS_Singularity`, would be the final profile to execute the fully developed workflow on the HPC. The HPC requires both a queue and a project code in order to use PBS scheduler. The [cluster options](https://www.nextflow.io/docs/latest/config.html#scope-process) must be configured to include the required `-P <project_code>` flag and `process.queue` is a process directive that can be defined directly. 

```
//Create profiles to easily switch between the different process executors and platforms.
profiles {
    //For running on an interactive session with singularity module loaded
    cybertron_singularity {
        process.executor = 'local'
        singularity.enabled = true 
    }
    //For executing the jobs on the HPC cluster.
    PBS_singularity {
        process.executor = 'pbspro' 
        process.queue = "${params.queue}"
        process.clusterOptions = "-P ${params.project}"
        //process.clusterOptions = "-P \$(project project ${params.project})" //renders correctly in command.run script but schedule won't accept it.
        process.beforeScript = 'module load singularity/3.9.9'
        singularity.enabled = true //need some logic here to switch to false if conda used
    }
    //For running interactively on local macbook with docker installed. 
    local_docker {
        process.executor = 'local'
        docker.enabled = true
    }
}
```

# Cleaning up Cached Data

Nextflow has an utility to [clean](https://www.nextflow.io/docs/latest/cli.html#clean) up old work directories and logs that are no longer needed. This can be run after x amount of time to keep your workdir from getting too large or if you're running out of disk space. 

This requires the session ID or session name, which can found in the following file 

```{bash}
cat .nextflow/history
nextflow clean -f high_sinoussi
```

# Session Information

```{r}
sessionInfo()
```



