---
title: "T-ALL Bulk RNA-seq STAR Quantification"
author: "Jenny L. Smith"
params:
    extra_path: "~/github_repos/R/linux-ubuntu-noble/R-4.5/x86_64-pc-linux-gnu"
---

# Set-up 

```{r}
library(biomaRt)
library(dplyr)
library(stringr)
library(purrr)
library(tidyr)
```

# Background

### Sequencing and Library Prep

**Protocol and the kit**

Our library prep uses random hexamers for cDNA synthesis. 

we've used (our RNA input was 100 ng): 

https://support.illumina.com/content/dam/illumina-support/documents/documentation/chemistry_documentation/illumina_prep/RNA/illumina-stranded-total-rna-prep-reference-guide-1000000124514-02.pdf 

Trimming T-Overhang 
https://knowledge.illumina.com/software/general/software-general-reference_material-list/000003227

The trimming can be done during or after FASTQ generation.


**Library Loaded**

100ng RNA input into the library preparation  

PhIX Control Library (https://emea.illumina.com/products/by-type/sequencing-kits/cluster-gen-sequencing-reagents/phix-control-v3.html): 

derived from the small, well-characterized bacteriophage genome, PhiX. It is a concentrated Illumina library (10 nM in 10 µl) that has an average size of 500 bp and consists of balanced base composition at ~45% GC and ~55% AT. 

We added PhIX at 1% of molarity to the pool of indexed libraries right before sequencing on NovaSeq 6000. 


**RNA yield per million cells** 

Please find attached the updated Excel file with both the number of cells per vial and the RNA yield (ng of extracted RNA per million cells) - it is very variable, and mostly depended on the viability we found when we thawed the cells. 


**External resources**

Teachey's Nature paper on T-ALL: https://www.nature.com/articles/s41586-024-07807-0 

Data availability (genomic data from 1,300 T-ALL samples analyzed in Teachey's paper): 

https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id=phs002276.v3.p1 

https://portal.kidsfirstdrc.org/dashboard 

Part of the code is available here: https://github.com/ppolonen/genomic_basis_TALL 

### Collaborators

- [NAME 1]
- [NAME 2]
- [...]

- Department of Pediatric Hematology-Oncology, and Cell and Gene Therapies

- Bambino Gesù Children's Hospital

# Install Dependencies

Use the lab quarto template 
```{bash}
#| eval: false


quarto use template Meshinchi-Lab/py_analysis_template
python3 -m venv venv 
source venv/bin/activate 
python3 -m pip install -r requirements.txt
```

Install nextflow and NF-Core tools CLI 
```{bash}
#| eval: false


python3 -m pip install nextflow
python3 -m pip install nf-core
```

# Download and Prepare Reference Data

```{bash}
#| eval: false


# URL='https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_48/GRCh38.primary_assembly.genome.fa.gz'

REFS="/genome_references/human/gencode_v48/"
sudo mkdir -p $REFS

rclone lsd --http-url https://ftp.ebi.ac.uk :http:/pub/databases/gencode/Gencode_human/release_48

cd $REFS
sudo rclone copy --http-url https://ftp.ebi.ac.uk :http:/pub/databases/gencode/Gencode_human/release_48/GRCh38.primary_assembly.genome.fa.gz . -P
```

```{r}
species <- "Homo_sapiens"
# https://www.gencodegenes.org/human/releases.html
# gencode v48 corresponds to ensembl v114
mart <- useEnsembl('ensembl', 
                   dataset = 'hsapiens_gene_ensembl', 
                   version = 114)
# list attributes 
mart_attr <- listAttributes(mart)
mart_filt <- listFilters(mart, what = c("name",
                                        "description",
                                        "fullDescription",
                                        "options"))

head(mart_attr)
head(mart_filt)

# Biotypes available 

# artifact,IG_C_gene,IG_C_pseudogene,IG_D_gene,IG_J_gene,IG_J_pseudogene,IG_pseudogene,IG_V_gene,IG_V_pseudogene,lncRNA,miRNA,misc_RNA,Mt_rRNA,Mt_tRNA,nonsense_mediated_decay,non_stop_decay,processed_pseudogene,processed_transcript,protein_coding,protein_coding_CDS_not_defined,protein_coding_LoF,pseudogene,retained_intron,ribozyme,rRNA,rRNA_pseudogene,scaRNA,snoRNA,snRNA,sRNA,TEC,transcribed_processed_pseudogene,transcribed_unitary_pseudogene,transcribed_unprocessed_pseudogene,translated_processed_pseudogene,TR_C_gene,TR_D_gene,TR_J_gene,TR_J_pseudogene,TR_V_gene,TR_V_pseudogene,unitary_pseudogene,unprocessed_pseudogene,vault_RNA
```

```{r}
rRNAs <- biomaRt::getBM(values=c("rRNA", "rRNA_pseudogene","Mt_rRNA","ribozyme"),
               filters="biotype", 
               attributes=c("chromosome_name","exon_chrom_start",
                            "exon_chrom_end","strand","ensembl_transcript_id",
                            "ensembl_exon_id","external_gene_name",
                            "gene_biotype","description"), 
               mart = mart)

head(rRNAs)
# table(rRNAs$strand)
dim(rRNAs) #589   

rRNAs_df <- rRNAs %>% 
  mutate(strand = ifelse(strand == 1, "+", "-"),
         start = exon_chrom_start,
         end = exon_chrom_end,
         score = 0,
         itemRgb = 0,
         blockCount = 1,
         blockSizes = exon_chrom_end - exon_chrom_start,
         blockStarts = 0) %>% 
  select(chromosome_name:exon_chrom_end,
         ensembl_exon_id,
         score, 
         strand,
         start,
         end,
         itemRgb,
         blockCount,
         blockSizes,
         blockStarts)

head(rRNAs_df)
dim(rRNAs_df)
# View(rRNAs)
```

```{r}
#| eval: false


dir.create("data/references/human/", recursive = TRUE)
write.table(select(rRNAs, ensembl_transcript_id),
            file = "data/references/human/ensembl_transcript_id_v114.txt",
            sep = "\t", 
            quote = FALSE, 
            col.names = FALSE,
            row.names = FALSE)
```

```{bash}
#| eval: false

sudo cp data/references/human/ensembl_transcript_id_v114.txt /genome_references/human/GRCh38/rRNA/ensembl_transcript_id_v114.txt
```

# Download Test Datasets

Human data from the NF-Core github repo 

```{bash}
#| eval: false

# https://github.com/nf-core/test-datasets/tree/rnafusion/testdata/human

sudo wget -v https://raw.githubusercontent.com/nf-core/test-datasets/refs/heads/rnafusion/testdata/human/README.md

sudo wget -v https://github.com/nf-core/test-datasets/raw/refs/heads/rnafusion/testdata/human/reads_1.fq.gz

sudo wget -v https://github.com/nf-core/test-datasets/raw/refs/heads/rnafusion/testdata/human/reads_2.fq.gz
```

# Run Test Data

### Set-up environment 

```{bash}
#| eval: false

# colima
cd ~/opt
curl -LO https://github.com/abiosoft/colima/releases/latest/download/colima-$(uname)-$(uname -m)
sudo install colima-$(uname)-$(uname -m) /usr/local/bin/colima

# lima
cd ~/opt
wget https://github.com/lima-vm/lima/releases/download/v1.1.1/lima-1.1.1-Linux-x86_64.tar.gz
tar -xvzf lima-1.1.1-Linux-x86_64.tar.gz

# qemu 1
sudo apt-get install qemu-system


# configure the docker engine
URL="https://download.docker.com/linux/ubuntu/dists/jammy/pool/stable/amd64"
wget $URL/containerd.io_1.7.27-1_amd64.deb
wget $URL/docker-ce-cli_28.3.1-1~ubuntu.22.04~jammy_amd64.deb
wget $URL/docker-ce_28.3.1-1~ubuntu.22.04~jammy_amd64.deb

sudo dpkg --log=containerd.log --debug=10 -i containerd.io_1.7.27-1_amd64.deb && \
sudo dpkg --log=docker-ce-cli.log --debug=10 -i docker-ce-cli_28.3.1-1~ubuntu.22.04~jammy_amd64.deb && \
sudo dpkg --log=docker-ce-cli.log --debug=10 -i docker-ce_28.3.1-1~ubuntu.22.04~jammy_amd64.deb
```

Configure the `docker` user group

```{bash}
#| eval: false


#https://docs.docker.com/engine/install/linux-postinstall/
sudo groupadd docker
sudo usermod -aG docker $USER
newgrp docker
docker run hello-world
```

```{bash}
#| eval: false


# activate venv
source venv/bin/activate
./main_run test_pe_dataset
```

getent group docker
id -u
id -g
id -G


# Run T-ALL Samples 

```{bash}
#| eval: false

SOURCE="$HOME/mnt/opbg/ngsonco/NGS/Francesca Benini/T-ALL/WTS run 2025.05.12/WTS_1-51-38360350"
TARGET="$HOME/github_repos/2025.06.02_RNAseq_Bulk_T-ALL/data/fastq/WTS_run_2025.05.12"

# symlink the fastqs from the network drive to the bioinformatics PC
ln -s "$SOURCE" $TARGET
```

```{bash}
# create an output directory for the rna-seq counts 
OUTDIR="$HOME/github_repos/2025.06.02_RNAseq_Bulk_T-ALL/data/rnaseq_quant"
mkdir $OUTDIR
```

# Sample Metadata

```{r}
sample_info <- read.table(file.path(Sys.getenv("HOME"),"github_repos/2025.06.02_RNAseq_Bulk_T-ALL/data/metadata/2025.06.03_T-ALL_samples_forJenny_rnaseq_libraries_cleaned.tsv"),
                          sep = "\t",
                          header = TRUE)


head(sample_info)
dim(sample_info)
```

## Concatenate Fastq Files 

```{r}
# list the fastq files for each sample/lane
fqs_path <- file.path(Sys.getenv("HOME"), "github_repos/2025.06.02_RNAseq_Bulk_T-ALL/data/fastq/WTS_run_2025.05.12")

if ( ! file.exists("fastq_filepaths.txt")){
 fqs_dir <- dir(fqs_path, 
               recursive = TRUE,
               full.names = TRUE,
               pattern = "*.fastq.gz") %>%
               as.data.frame()
  write.delim(fqs_dir, "fastq_filepaths.txt", sep="\t", col.names = FALSE, row.names = FALSE)
}

fqs_dir <- read.delim("fastq_filepaths.txt", header=FALSE)
fqs_dir <- fqs_dir[["V1"]]
head(fqs_dir)
length(fqs_dir) #204
```

```{r}
# define outdirs
OUTDIR="/mnt/bioinformatics/nextflow_out/2025.06.02_RNAseq_Bulk_T-ALL/fastq"
PROJ_DATA_DIR <- file.path(Sys.getenv("HOME"), "github_repos/2025.06.02_RNAseq_Bulk_T-ALL/data/fastq/concatenated")
dir.create(PROJ_DATA_DIR, recursive = TRUE)

# concatenate Lane 1 and Lane 2 for all samples
fqs_concat<- map_dfr(.x = fqs_dir, .f = ~ t(str_split_1(.x, pattern = "/")) %>% 
    as.data.frame()) %>% 
    select(-V1) 

fqs_concat <- fqs_concat %>% 
    rename_all(.funs = ~ unlist(fqs_concat[1,]))  %>% 
    mutate(fastq_path = fqs_dir,
           single_end = "false",
           library_chemistry = "Illumina Stranded Total RNA Prep, Ligation with Ribo-Zero Plus") %>% 
    select(-c(home:github_repos, data),
            project = `2025.06.02_RNAseq_Bulk_T-ALL`,
            type = fastq,
            run_name = WTS_run_2025.05.12,
            fastq_generation = `FASTQ_Generation_2025-05-15_14_48_36Z-60284247`,
            lane = `47_L002-ds.f8598a51d9c442ffb20db6d63e4e182d`,
            fastq_file = `6728_S47_L002_R1_001.fastq.gz`,
            fastq_path) %>% 
    mutate(sample_number = str_split_fixed(lane, pattern = "_", n = 2)[,1],
           lane = str_split_fixed(lane, pattern = "[_-]", n = 3)[,2], 
           sample_name = str_split_fixed(fastq_file, pattern = "_", n = 2)[,1],
           read = str_split_fixed(fastq_file, pattern = "_", n = 5)[,4],
           sample_id = gsub("^(.+)_L.+001.+$", "\\1", fastq_file)) %>% 
    select(sample_id, 
           sample_name, 
           lane, 
           read, 
           fastq_file, 
           everything()) %>% 
    arrange(desc(sample_id), read, lane) %>% 
    group_by(sample_id, sample_name, read) %>%
    mutate(concatenated_fastq = glue::glue("{OUTDIR}/{sample_id}_{read}_001.fastq"),
           concatenated_fastq_path = glue::glue("{OUTDIR}/{sample_id}_{read}_001.fastq.gz")) %>%
    mutate(cat_command = glue::glue("echo 'Processing {sample_id}_{read}'\nzcat {paste(fastq_path, collapse = ' ')} > {concatenated_fastq} && gzip -v {concatenated_fastq} & \n") %>%
               unique() %>% 
               c(.,"")) %>%
    ungroup() %>%
    select(-concatenated_fastq)

head(fqs_concat)
# dim(fqs_concat) #204  12
# length(unique(fqs_concat$sample_name)) #51

write.table(fqs_concat, 
            file = "data/2025.06.02_RNAseq_Bulk_T-ALL_fastq_manifest.tsv",
            sep = "\t")
```

```{bash}
#| eval: false

# Use the partition in /mnt with ~4tb of disk space
OUTDIR="/mnt/bioinformatics/nextflow_out/2025.06.02_RNAseq_Bulk_T-ALL/fastq"
# make a symlink to the data analysis directory
PROJ_DATA_DIR="$HOME/github_repos/2025.06.02_RNAseq_Bulk_T-ALL/data/fastq/concatenated"

ln -s $OUTDIR $PROJ_DATA_DIR
```

```{r}
#| eval: false

PROJ_DATA_DIR <- file.path(Sys.getenv("HOME"), "github_repos/2025.06.02_RNAseq_Bulk_T-ALL/data/fastq/concatenated")

# create a single shell script to concatenate Lane 1 and Lane 2 fastqs for each sample
cmds <- fqs_concat[["cat_command"]] %>% 
    grep("^$", ., invert = TRUE, value = TRUE)

length(cmds) #102

# split into two shell scripts to have a small initial sample size of N=3 to estimate necessary computational resources
cmds_list <- map(1:2, .f = function(i, all_cmds = cmds){
  if (i == 1){
    index <- 1:6
  }else{
    index <- 7:length(all_cmds)
  }
  cmd_string <- all_cmds[index] %>%
    paste(., collapse = "\n") %>% 
    glue::glue("#!/bin/bash\n\n",
          "# uncompress and concatentate fastq files from lane 1 and lane 2\n",
           "{cmds}\n",
           "wait\n\n",
           cmds = .)
  
  # write the shell scripts to a file in the analysis directory
  outfile <- paste0("concat_fqs_v",i,".sh")
  cat(cmd_string, file = file.path(dirname(PROJ_DATA_DIR),outfile))

  # return the string
  cmd_string
})

# head(cmds)
str_count(string = cmds_list[[1]], pattern = "zcat") # 6
str_count(string = cmds_list[[2]], pattern = "zcat") # 6
```

```{bash, eval=FALSE}
#| eval: false

# define variables
FQS_DIR="$HOME/github_repos/2025.06.02_RNAseq_Bulk_T-ALL/data/fastq"
SCRIPT="$FQS_DIR/concat_fqs_v1.sh"
LOG="$FQS_DIR/concatenation_v1.log"

# Run the concatenation script. save stdout and stderr to log file
bash $SCRIPT >> $LOG 2>&1
```


## Sample Sheet 

```{r}
#| eval: false


DESCRIPTION_HEADER <- c("# column r1 : the absolute or relative filepath for the read 1 fastq in paired-end RNA-seq, or the single-end fastq file.
# column r2 : the absolute or relative filepath for the read 2 fastq in paired-end RNA-seq.
# column id : unique sample ID, no duplicates allowed in the sample sheet.
# column single_end : boolean [true/false] if the data is single-end or paired-end.
id,r1,r2,single_end")

SAMPLE_SHEET <- "data/sample_sheets/2025.06.03_T-ALL_bulk_rnaseq_sample_sheet_"
V1 <- paste0(SAMPLE_SHEET, "v1.csv")
V2 <- paste0(SAMPLE_SHEET,"v2.csv")

cat(DESCRIPTION_HEADER, file = V1, sep = "\n")
cat(DESCRIPTION_HEADER, file = V2, sep = "\n")

sample_sheet <- fqs_concat %>% 
  select(id = sample_id,
          read,
          single_end,
          concatenated_fastq_path) %>% 
  distinct() %>% 
  pivot_wider(id_cols = c(id, single_end),
              names_from = read,
              values_from = concatenated_fastq_path) %>% 
  janitor::clean_names()
              

head(sample_sheet)
dim(sample_sheet)

for (i in 1:2) {
  outfile <- paste0("V",i)
  if (i == 1){
    index <- c(1:3)
  }else{
    index <- c(4:51)
  }

  sample_sheet[index,] %>% 
    glue::glue_data("{id},{r1},{r2},{single_end}")  %>% 
    cat(.,  file = get(outfile), sep = "\n", append = TRUE)

}
```

```{bash}
#| eval: false


NXF_OUTDIR="/mnt/bioinformatics/nextflow_out/2025.06.02_RNAseq_Bulk_T-ALL/rnaseq_quant"
PROJ_OUTDIR="$HOME/github_repos/2025.06.02_RNAseq_Bulk_T-ALL/data/rnaseq_quant"

ln -s $NXF_OUTDIR $PROJ_OUTDIR
```

```{bash}
#| eval: false

tmux 
source venv/bin/activate
./main_run.sh 2025.06.02_RNAseq_Bulk_T-ALL_V1
./main_run.sh 2025.06.02_RNAseq_Bulk_T-ALL_V2
```

# Computational Resources


AWS Regions

https://docs.aws.amazon.com/global-infrastructure/latest/regions/aws-regions.html

```{r}
aws_cost_estimator <- read.csv("~/github_repos/Amazon_EC2_Instances_BulkUpload_Template_Commercial.csv") %>% 
    janitor::clean_names() %>% 
    select(-x)

head(aws_cost_estimator)
```

```{r}
regions <- read.delim("~/github_repos/AWS_EU_Regions.txt") %>% 
    janitor::clean_names()

head(regions)
regions
```

For 1 sample in bulk RNA-seq pipeline

For each job: CPUs, RAM
For each region:
    - generate instance types from requirements 
    
then final loop:
    - For each job: CPUs, RAM
    - For each region:
    - for each instance type

```{bash}
#| eval: false
#| 
aws ec2 get-instance-types-from-instance-requirements \
    --region us-east-1 \
    --generate-cli-skeleton input > attributes.json
```

```{r}
instance_type_attr <- jsonlite::read_json("attributes.json")

instance_type_attr_df <- as.data.frame(instance_type_attr)
```

```{r}
trace_file <- read.delim("reports/2025-06-02_RNAseq_Bulk_T-ALL_trim_subread_prepFusions_2025-09-08_trace.txt") %>% 
    mutate(req_cpus = case_when(
        grepl("FASTQC|INDEX", name) ~ 2,
        grepl("TRIMGALORE|STAR|PICARD|SUBREAD", name) ~ 4,
        TRUE ~ 1)) %>% 
    mutate(sample_id = stringr::str_split_fixed(name, pattern = "\\s", n = 2)[,2] %>% 
               gsub("\\(|\\)","",.)) %>% 
    group_by(sample_id) %>% 
    mutate(job_number = 1:dplyr::n()) %>% 
    ungroup() %>% 
    mutate(job_name = gsub(":", "_", gsub("^(.+)\\s.+", "\\1", name)),
           peak_vmem_mib = fs::fs_bytes(peak_vmem) %>% 
               R.utils::hsize(., units = "MiB", digits = 0)) %>% 
    select(sample_id, job_name,job_number, peak_vmem_mib, 
           name,req_cpus, everything())

head(trace_file)
trace_file
# dim(trace_file)
```

```{r}
resources_N1 <- trace_file %>% 
    filter(grepl("TALL-1_S1", name)) 

resources_N1
# write.csv(resources_N1, "data/computational_resources/resources_N1.csv", row.names = FALSE)
```

```{r}
resources_json <- purrr::map2(.x = pull(resources_N1, job_number, name = job_name),
                              .y = resources_N1$job_name,
                              .f = function(job,jobname) {
    
    print(job)
    task <- resources_N1[job,]
    
    ec2_requirements <- instance_type_attr
    ec2_requirements$ArchitectureTypes <- "x86_64"
    ec2_requirements$VirtualizationTypes <- "hvm"
    ec2_requirements$InstanceRequirements$VCpuCount$Min <- task[["req_cpus"]]
    ec2_requirements$InstanceRequirements$VCpuCount$Max <- task[["req_cpus"]]
    ec2_requirements$InstanceRequirements$MemoryMiB$Max <- task[["peak_vmem_mib"]]
    ec2_requirements <- ec2_requirements[c("ArchitectureTypes","VirtualizationTypes","InstanceRequirements")]
    ec2_requirements <- ec2_requirements$InstanceRequirements[""]
    
    # ec2_requirements <- jsonlite::toJSON(ec2_requirements)
    
    # jsonlite::write_json(ec2_requirements,
                         # path = glue::glue("data/computational_resources/{jobname}_aws_ec2_instance_reqs_per_job.json"))
    return(ec2_requirements)
})

# jsonlite::write_json(resources_json, path = "data/aws_ec2_instance_reqs_per_job.json")
resources_json$rnaseq_count_FASTQC
```

Configure the JSON file. You must provide values for ArchitectureTypes, VirtualizationTypes, VCpuCount, and MemoryMiB.

```{r}
colnames(aws_cost_estimator)
# instance_type_attr$VirtualizationTypes
```

```{bash}
aws ec2 get-instance-types-from-instance-requirements \
    --region 'eu-south-1' \
    --cli-input-json file://data/computational_resources/rnaseq_count_SUBREAD_FEATURECOUNTS_aws_ec2_instance_reqs_per_job.json \
    --output table
```



```{bash}
#| eval: false

cat reports/2025.06.02_RNAseq_Bulk_T-ALL_V2_2025-07-15.html | grep -E "window.data.+trace" -A 1 > reports/2025.06.02_RNAseq_Bulk_T-ALL_V2_2025-07-15.nextflow_trace.json
```

```{r}
library(jsonlite)

resources <- fromJSON("reports/2025.06.02_RNAseq_Bulk_T-ALL_V2_2025-07-15.nextflow_trace.json")
head(resources)
```

```{r}
resources[resources$task_id == 101, ]
```

```{r}
memory_gb <- R.utils::hsize(sum(as.numeric(resources[["memory"]]), na.rm = TRUE), units = "GB")
memory_gb

cpus <- sum(as.numeric(resources[["cpus"]]), na.rm=TRUE)
cpus
```



# Session Information

```{python}
session_info.show()
```
